#!/bin/bash
#SBATCH --partition=debug
#SBATCH --time=00:20:00
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
#SBATCH --exclusive
#SBATCH --job-name="test_hive"
#SBATCH --output=test-%J.out
#SBATCH --mail-user=vannapur@buffalo.edu
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.
#
#This SLURM script is modified version of the SDSC script
# found in /util/academic/myhadoop/myHadoop-0.30b/examples.
# CDC January 29, 2015
#
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR

echo "working directory = "$SLURM_SUBMIT_DIR

module load java/1.6.0_22
module load hadoop/2.5.1
module load hive/0.14.0
module load myhadoop/0.30b
module list
echo "MH_HOME="$MH_HOME
echo "HADOOP_HOME="$HADOOP_HOME
echo "Setting HADOOP to use SLURMTMPDIR on the local disk"
export MH_SCRATCH_DIR=$SLURMTMPDIR
echo "MH_SCRATCH_DIR="$MH_SCRATCH_DIR
#### Set this to the directory where Hadoop configs should be generated
# Don't change the name of this variable (HADOOP_CONF_DIR) as it is
# required by Hadoop - all config files will be picked up from here
#
# Make sure that this is accessible to all nodes
export HADOOP_CONF_DIR=$SLURM_SUBMIT_DIR/config-$SLURM_JOBID
export HIVE_CONF_DIR=$SLURM_SUBMIT_DIR/config-$SLURM_JOBID
echo "create diretory for HIVE metadata"
### Set up the configuration
# Make sure number of nodes is the same as what you have requested from PBS
# usage: $myhadoop-configure.sh -h
# this is the non-persistent mode
NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo "-------Set up the configurations for myHadoop"
$MH_HOME/bin/myhadoop-configure.sh 
#
cp $HIVE_HOME/conf/hive-env.sh-sample $HIVE_CONF_DIR/hive-env.sh
cp $HIVE_HOME/conf/hive-default.xml-sample $HIVE_CONF_DIR/hive-default.xml
sed -i 's:MY_HIVE_SCRATCH:'"$SLURMTMPDIR"':g' $HIVE_CONF_DIR/hive-default.xml
cp $HIVE_HOME/conf/hive-log4j.properties-sample $HIVE_CONF_DIR/hive-log4j.properties
sed -i 's:MY_HIVE_DIR:'"$SLURM_SUBMIT_DIR"':' $HIVE_CONF_DIR/hive-log4j.properties
ls -l $HADOOP_CONF_DIR
echo "-------Start hdfs and yarn ---"
$HADOOP_HOME/sbin/start-all.sh
#### Format HDFS, if this is the first time or not a persistent instance
echo "-------Show Report ---"
#$HADOOP_HOME/bin/hadoop dfsadmin -report
echo "-------make directory ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -mkdir /tmp
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -mkdir -p /user/hive/warehouse
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -chmod g+w /tmp
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -chmod g+w /user/hive/warehouse
echo "-------list warehouse directory ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /user/hive/warehouse

echo "Create database vannapur_stocks"
$HIVE_HOME/bin/hive -e "DROP TABLE IF EXISTS stock_vol;"

echo "show database"
$HIVE_HOME/bin/hive -e "SHOW DATABASES;"

$HIVE_HOME/bin/hive -e "CREATE TABLE stock_vol(
date STRING,
open FLOAT,
high FLOAT,
low FLOAT,
close FLOAT,
volume INT,
adjClose FLOAT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;"

echo "Load data"
$HIVE_HOME/bin/hive -e "LOAD DATA LOCAL INPATH '$1/*.csv' OVERWRITE INTO TABLE stock_vol;"

echo "show database"
$HIVE_HOME/bin/hive -e "SHOW DATABASES;"

echo "cleaning raw data"
$HIVE_HOME/bin/hive -e "CREATE TABLE A AS select INPUT__FILE__NAME,split(date, '-')[0] as year,split(date, '-')[1] as month,split(date, '-')[2] as day,adjclose from stock_vol where date !='Date';"

echo "getting opening and closing days for month"
$HIVE_HOME/bin/hive -e "CREATE TABLE MM_DAY AS select INPUT__FILE__NAME as file,year as year,month as month,MAX(day) AS lastday,MIN(day) as firstday from A GROUP BY INPUT__FILE__NAME,year,month;"

echo "getting last day adjClose value for each month"
$HIVE_HOME/bin/hive -e "CREATE TABLE MAXPRICE AS Select A.input__file__name as file,A.month as month,A.year as year,A.adjclose as val from A join MM_DAY on A.input__file__name = MM_DAY.file AND A.month = MM_DAY.month AND A.year = MM_DAY.year where A.day = MM_DAY.lastday;"

echo "getting last day adjClose value for each month"
$HIVE_HOME/bin/hive -e "CREATE TABLE MINPRICE AS Select A.input__file__name as file,A.month as month,A.year as year,A.adjclose as val from A join MM_DAY on A.input__file__name = MM_DAY.file AND A.month = MM_DAY.month AND A.year = MM_DAY.year where A.day = MM_DAY.firstday;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE A;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE MM_DAY;"

echo "getting opening and closing adjClose values for each month"
$HIVE_HOME/bin/hive -e "CREATE TABLE OCVALS AS Select MAXPRICE.file as file, MAXPRICE.year as year, MAXPRICE.month as month, MAXPRICE.val as last, MINPRICE.val as first from MAXPRICE join MINPRICE on MAXPRICE.file=MINPRICE.file AND MAXPRICE.year=MINPRICE.year AND MAXPRICE.month=MINPRICE.month;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE MAXPRICE;"
echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE MINPRICE;"

echo "Generating Xi value for each month"
$HIVE_HOME/bin/hive -e "CREATE TABLE XIVALUE AS SELECT file, year, month, (last-first)/first as x_i from OCVALS;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE OCVALS;"

echo "Generating AVG(Xi) value for each month"
$HIVE_HOME/bin/hive -e "CREATE TABLE T3 AS Select XIVALUE.file as file, AVG(x_i) as x_bar from XIVALUE GROUP BY XIVALUE.file;"

echo "Combining Xi and AVG(Xi) together"
$HIVE_HOME/bin/hive -e "CREATE TABLE XIXBAR AS SELECT XIVALUE.file,XIVALUE.year,XIVALUE.month, XIVALUE.x_i, t3.x_bar from t3 join XIVALUE on t3.file = XIVALUE.file;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE XIVALUE;"
echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE T3;"

echo "Calculating Volatility Index"
$HIVE_HOME/bin/hive -e "CREATE TABLE vol AS select XIXBAR.file as stock, sqrt(SUM((x_i-x_bar)*(x_i-x_bar))/(Count(*)-1)) as volatility from XIXBAR GROUP by file;"

echo "show database"
$HIVE_HOME/bin/hive -e "DROP TABLE XIXBAR;"

echo "Top 10 Stocks with Maximum Volatility"
$HIVE_HOME/bin/hive -e "select * from vol where volatility IS NOT NULL AND volatility >0.0 ORDER BY volatility DESC LIMIT 10;"

echo "Top 10 Stocks with Minimum Volatility"
$HIVE_HOME/bin/hive -e "select * from vol where volatility IS NOT NULL AND volatility >0.0 ORDER BY volatility ASC LIMIT 10;"

echo "-------Stop hdfs and yarn ---"
$HADOOP_HOME/sbin/stop-all.sh

#### Clean up the working directories after job completion
$MH_HOME/bin/myhadoop-cleanup.sh

